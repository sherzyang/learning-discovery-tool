{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_dump_reader import Cleaner, iterate\n",
    "\n",
    "corpus = []\n",
    "cleaner = Cleaner()\n",
    "for title, text in iterate('data/enwiki-latest-pages-articles.xml'):\n",
    "    text = cleaner.clean_text(text)\n",
    "    cleaned_text, links = cleaner.build_links(text)\n",
    "    corpus.append(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = []\n",
    "for i in corpus:\n",
    "    if len(i) > 250:\n",
    "        long_text.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text_count = len(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sherzyang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = long_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize by words in docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Louis', 'J.', 'Russell', '(circa', '1912–1973)', 'was', 'an', 'agent', 'of', 'the', 'Federal', 'Bureau', 'of', 'Investigation', '(FBI),', 'senior', 'investigator', 'of', 'the', 'House', 'Un-American', 'Activities', 'Committee', '(HUAC),', 'and', 'private', 'detective.\\n==Background==\\nLouis', 'James', 'Russell', 'graduated', 'from', 'the', 'Catholic', 'University', 'of', 'America.\\n==Career==\\nIn', '1935,', 'Russell', 'joined', 'the', 'FBI', 'as', 'an', 'agent.\\n===HUAC===\\nIn', '1945,', 'Russell', 'joined', 'HUAC', 'as', 'an', 'investigator.', '', 'Robert', 'E.', 'Stripling', 'has', 'Russell', 'testify', 'on', 'what', 'he', 'knew', 'about', 'Gerhart', 'Eisler', 'and', 'Hollywood', 'industry', 'people.', '', 'He', 'also', 'testified', 'about', 'Leon', 'Josephson', 'and', 'Alexander', 'Koral.', '', 'By', '1948,', 'Russell', 'was', 'a', 'HUAC', 'senior', 'investigator', 'in', 'the', 'Hiss-Chambers', 'case.', '', 'In', 'his', 'memoir', 'Six', 'Crises,', 'Richard', 'Nixon', 'recalled', 'that', 'Russell', 'restrained', 'Hiss', 'when', 'it', 'seemed', 'Hiss', 'was', 'about', 'to', 'strike', 'Chambers.', '', 'Russell', 'served', 'under', 'Robert', 'E.', 'Stripling', 'and', 'his', 'successor', 'Frank', 'S.', 'Tavenner', 'Jr..', '', 'He', 'helped', 'uncover', 'evidence', 'of', 'Soviet', 'spy', 'rings', 'and', 'leaks', 'of', 'atomic', 'secrets', 'and', 'materials', 'to', 'the', 'Soviet', 'Union.', '', 'In', '1952,', 'he', 'helped', 'try', 'to', 'find', 'Communist', 'influence', 'in', 'the', 'motion', 'picture', 'industry.\\nIn', 'January', '1954,', 'Russell', 'was', 'dismissed', 'by', 'committee', 'chair,', 'Representative', 'Harold', 'H.', 'Velde.', '', 'Russell', 'had', 'borrowed', '$300', 'from', 'actor', 'Edward', 'G.', 'Robinson.', '', 'In', '1956,', 'Russell', 'was', 'rehired', 'and', 'remained', 'with', 'HUAC', 'for', 'a', 'decade.\\n===Private', 'detective===\\nIn', '1966,', 'Russell', 'became', 'a', 'private', 'investigator.\\nTo', 'undermine', 'the', 'credibility', 'of', 'investigative', 'report', 'Jack', 'Anderson,', 'the', '', 'Richard', 'M.', 'Nixon', 'campaign', 'hired', 'Russell', '\"to', 'spy\"', 'on', 'him.', '', 'In', 'return', 'for', 'leads,', 'Anderson', 'gave', 'Russell', 'odd', 'jobs', 'for', 'the', '\"Washington', 'Merry-Go-Round,\"', 'enabling', 'Russell', 'to', 'send', 'information', 'back', 'to', 'the', 'campaign,', 'whose', 'director', 'of', 'security', 'was', 'James', 'W.', 'McCord\\nIn', '1971,', 'Russell', 'was', 'working', 'for', 'General', 'Security', 'Services,', 'Inc.', '(GSS),', 'a', 'security', 'guard', 'service', 'whose', 'clients', 'included', 'the', 'Watergate', 'offices.', '', 'After', 'the', 'Watergate', 'break-in', 'in', '1972,', 'James', 'W.', 'McCord', 'Jr.', '\"refused', 'to', 'discuss', 'Russell', 'under', 'any', 'circumstances', 'and...', 'would', 'not', 'discuss', 'Watergate', 'with', 'any', 'writer', 'who', 'so', 'much', 'as', 'expressed', 'interest', 'in', 'Lou', 'Russell.\"\\nFrom', 'June', '20', 'to', 'July', '2,', '1973,', 'Russell', 'was', 'working', 'for', 'a', 'detective', 'agency', 'that', 'was', 'helping', 'George', 'Herbert', 'Walker', 'Bush', '(then', 'chairman', 'of', 'the', 'Republican', 'National', 'Committee)', 'prepare', 'for', 'a', 'press', 'conference.\\nAccording', 'to', 'attorney', 'Gerald', 'Alch,', 'McCord', 'hired', '\"an', 'old', 'associate', 'of', 'his\"', '(Russell)', 'to', 'his', 'company', 'Security', 'International,', 'Inc.', '', 'Bob', 'Smith,', 'aide', 'and', 'office', 'manager', 'to', 'attorney', 'Bernard', 'Fensterwald', 'recounted', 'that', 'McCord', 'had', 'obtained', 'a', 'contract', 'to', 'provide', 'security', 'to', 'the', 'Republican', 'National', 'Committee.', 'Unable', 'to', 'cash', \"McCord's\", 'checks,', 'Russell', 'brought', 'some', 'dozen', 'checks', 'over', 'time', 'to', 'Fensterwald’s', 'office', 'at', 'the', '\"Committee', 'to', 'Investigate', 'Assassinations\"', 'or', 'CTIA', '(1520', '16th', 'Street', 'NW,', 'Washington', 'DC', '20036),', 'which', 'Fensterwald', 'would', 'cash.', '', 'During', 'the', 'Watergate', 'break-in,', 'Russell', 'was', 'checked', 'into', 'a', 'Howard', \"Johnson's\", 'Motel', 'across', 'from', 'Watergate.\\n==Personal', 'life', 'and', 'death==\\nAuthor', 'Jim', 'Hougan', 'characterized', 'Russell', 'as', 'an', 'alcoholic', 'and', 'womanizer.\\nRussell', 'died', 'age', '61', 'on', 'July', '2,', '1973,', 'in', 'Washington,', 'DC,', 'after', 'a', 'heart', 'attack.\\n==Legacy==\\nIn', '1984,', 'Jim', 'Hougan', 'wrote', 'a', 'book', 'called', 'Secret', 'Agenda:', 'Watergate,', 'Deep', 'Throat,', 'and', 'the', 'CIA', 'that', 'started', 'originally', 'about', 'Russell.\\n==See', 'also==\\nJ.', 'Edgar', 'Hoover\\nWilliam', 'C.', 'Sullivan\\nManhattan', 'Project\\nAlger', 'Hiss\\nWhittaker', 'Chambers\\n==References==\\n==External', 'links==\\nArchive.org:', '1947', 'HUAC', 'Testimony', 'of', 'Louis', 'J.', 'Russell', '(pp.', '296-305,', '341-342)\\nArchive.org:', '1950', 'HUAC', 'Testimony', 'of', 'Louis', 'J.', 'Russell', '(pp.', '902-907)\\nNixon', 'Library:', '', 'Guide', 'to', 'the', 'Congressional', 'Papers', '(1947-1950)\\nCIA:', '1952', 'HUAC', 'Testimony', 'of', 'Walter', 'Bedell', 'Smith]]\\n1973', 'deaths\\nFBI', 'agents\\nCatholic', 'University', 'of', 'America', 'alumni']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['louis', 'russell', 'circa', 'agent', 'federal', 'bureau', 'investigation', 'senior', 'investigator', 'house', 'american', 'activities', 'committee', 'huac', 'private', 'detective', 'background', 'louis', 'jam', 'russell', 'graduate', 'catholic', 'university', 'america', 'career', 'russell', 'join', 'agent', 'huac', 'russell', 'join', 'huac', 'investigator', 'robert', 'stripling', 'russell', 'testify', 'know', 'gerhart', 'eisler', 'hollywood', 'industry', 'people', 'testify', 'leon', 'josephson', 'alexander', 'koral', 'russell', 'huac', 'senior', 'investigator', 'hiss', 'chamber', 'case', 'memoir', 'crises', 'richard', 'nixon', 'recall', 'russell', 'restrain', 'hiss', 'hiss', 'strike', 'chamber', 'russell', 'serve', 'robert', 'stripling', 'successor', 'frank', 'tavenner', 'help', 'uncover', 'evidence', 'soviet', 'ring', 'leak', 'atomic', 'secrets', 'materials', 'soviet', 'union', 'help', 'communist', 'influence', 'motion', 'picture', 'industry', 'january', 'russell', 'dismiss', 'committee', 'chair', 'representative', 'harold', 'velde', 'russell', 'borrow', 'actor', 'edward', 'robinson', 'russell', 'rehired', 'remain', 'huac', 'decade', 'private', 'detective', 'russell', 'private', 'investigator', 'undermine', 'credibility', 'investigative', 'report', 'jack', 'anderson', 'richard', 'nixon', 'campaign', 'hire', 'russell', 'return', 'lead', 'anderson', 'give', 'russell', 'job', 'washington', 'merry', 'round', 'enable', 'russell', 'send', 'information', 'campaign', 'director', 'security', 'jam', 'mccord', 'russell', 'work', 'general', 'security', 'service', 'security', 'guard', 'service', 'clients', 'include', 'watergate', 'offices', 'watergate', 'break', 'jam', 'mccord', 'refuse', 'discuss', 'russell', 'circumstances', 'discuss', 'watergate', 'writer', 'express', 'russell', 'june', 'july', 'russell', 'work', 'detective', 'agency', 'help', 'george', 'herbert', 'walker', 'bush', 'chairman', 'republican', 'national', 'committee', 'prepare', 'press', 'conference', 'accord', 'attorney', 'gerald', 'alch', 'mccord', 'hire', 'associate', 'russell', 'company', 'security', 'international', 'smith', 'aide', 'office', 'manager', 'attorney', 'bernard', 'fensterwald', 'recount', 'mccord', 'obtain', 'contract', 'provide', 'security', 'republican', 'national', 'committee', 'unable', 'cash', 'mccord', 'check', 'russell', 'bring', 'dozen', 'check', 'time', 'fensterwald', 'office', 'committee', 'investigate', 'assassinations', 'ctia', 'street', 'washington', 'fensterwald', 'cash', 'watergate', 'break', 'russell', 'check', 'howard', 'johnson', 'motel', 'watergate', 'personal', 'life', 'death', 'author', 'hougan', 'characterize', 'russell', 'alcoholic', 'womanizer', 'russell', 'die', 'july', 'washington', 'heart', 'attack', 'legacy', 'hougan', 'write', 'book', 'call', 'secret', 'agenda', 'watergate', 'deep', 'throat', 'start', 'originally', 'russell', 'edgar', 'hoover', 'william', 'sullivan', 'manhattan', 'project', 'alger', 'hiss', 'whittaker', 'chamber', 'reference', 'external', 'link', 'archive', 'huac', 'testimony', 'louis', 'russell', 'archive', 'huac', 'testimony', 'louis', 'russell', 'nixon', 'library', 'guide', 'congressional', 'paper', 'huac', 'testimony', 'walter', 'bedell', 'smith', 'deaths', 'agents', 'catholic', 'university', 'america', 'alumni']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for word in docs[0].split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = list(map(lambda x: preprocess(x), docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10102"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-26 16:51:28,610 : INFO : collecting all words and their counts\n",
      "2019-06-26 16:51:28,612 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-06-26 16:51:28,864 : INFO : PROGRESS: at sentence #10000, processed 1214001 words, keeping 95752 word types\n",
      "2019-06-26 16:51:28,868 : INFO : collected 96426 word types from a corpus of 1225961 raw words and 10102 sentences\n",
      "2019-06-26 16:51:28,868 : INFO : Loading a fresh vocabulary\n",
      "2019-06-26 16:51:28,952 : INFO : effective_min_count=5 retains 20958 unique words (21% of original 96426, drops 75468)\n",
      "2019-06-26 16:51:28,953 : INFO : effective_min_count=5 leaves 1110763 word corpus (90% of original 1225961, drops 115198)\n",
      "2019-06-26 16:51:29,007 : INFO : deleting the raw counts dictionary of 96426 items\n",
      "2019-06-26 16:51:29,009 : INFO : sample=0.001 downsamples 21 most-common words\n",
      "2019-06-26 16:51:29,010 : INFO : downsampling leaves estimated 1091580 word corpus (98.3% of prior 1110763)\n",
      "2019-06-26 16:51:29,067 : INFO : estimated required memory for 20958 words and 300 dimensions: 60778200 bytes\n",
      "2019-06-26 16:51:29,068 : INFO : resetting layer weights\n",
      "2019-06-26 16:51:29,360 : INFO : training model with 3 workers on 20958 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-06-26 16:51:30,437 : INFO : EPOCH 1 - PROGRESS: at 18.59% examples, 199668 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:31,465 : INFO : EPOCH 1 - PROGRESS: at 40.36% examples, 213342 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:32,478 : INFO : EPOCH 1 - PROGRESS: at 62.07% examples, 225714 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:33,496 : INFO : EPOCH 1 - PROGRESS: at 85.92% examples, 228854 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:34,017 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 16:51:34,018 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 16:51:34,035 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 16:51:34,035 : INFO : EPOCH - 1 : training on 1225961 raw words (1091628 effective words) took 4.7s, 233608 effective words/s\n",
      "2019-06-26 16:51:35,060 : INFO : EPOCH 2 - PROGRESS: at 21.29% examples, 235707 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:36,164 : INFO : EPOCH 2 - PROGRESS: at 45.82% examples, 235709 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:37,175 : INFO : EPOCH 2 - PROGRESS: at 63.89% examples, 229695 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:38,182 : INFO : EPOCH 2 - PROGRESS: at 85.15% examples, 224767 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:38,831 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 16:51:38,837 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 16:51:38,841 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 16:51:38,842 : INFO : EPOCH - 2 : training on 1225961 raw words (1091528 effective words) took 4.8s, 227224 effective words/s\n",
      "2019-06-26 16:51:39,881 : INFO : EPOCH 3 - PROGRESS: at 21.29% examples, 232603 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:40,888 : INFO : EPOCH 3 - PROGRESS: at 44.82% examples, 241175 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-26 16:51:41,917 : INFO : EPOCH 3 - PROGRESS: at 65.64% examples, 240297 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:42,919 : INFO : EPOCH 3 - PROGRESS: at 88.93% examples, 240603 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:43,320 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 16:51:43,329 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 16:51:43,347 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 16:51:43,348 : INFO : EPOCH - 3 : training on 1225961 raw words (1091625 effective words) took 4.5s, 242440 effective words/s\n",
      "2019-06-26 16:51:44,401 : INFO : EPOCH 4 - PROGRESS: at 21.29% examples, 229464 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:45,405 : INFO : EPOCH 4 - PROGRESS: at 41.26% examples, 222744 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:46,445 : INFO : EPOCH 4 - PROGRESS: at 61.47% examples, 224302 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:47,475 : INFO : EPOCH 4 - PROGRESS: at 83.97% examples, 223671 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:48,126 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 16:51:48,127 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 16:51:48,139 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 16:51:48,140 : INFO : EPOCH - 4 : training on 1225961 raw words (1091403 effective words) took 4.8s, 227920 effective words/s\n",
      "2019-06-26 16:51:49,163 : INFO : EPOCH 5 - PROGRESS: at 18.59% examples, 210132 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:50,258 : INFO : EPOCH 5 - PROGRESS: at 43.35% examples, 224502 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:51,295 : INFO : EPOCH 5 - PROGRESS: at 65.64% examples, 234121 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:52,341 : INFO : EPOCH 5 - PROGRESS: at 90.72% examples, 237581 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:51:52,659 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 16:51:52,667 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 16:51:52,687 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 16:51:52,688 : INFO : EPOCH - 5 : training on 1225961 raw words (1091442 effective words) took 4.5s, 240101 effective words/s\n",
      "2019-06-26 16:51:52,690 : INFO : training on a 6129805 raw words (5457626 effective words) took 23.3s, 233945 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(processed_docs, sg=1, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-26 16:52:11,825 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-06-26 16:52:11,826 : INFO : training model with 3 workers on 20958 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-06-26 16:52:12,832 : INFO : EPOCH 1 - PROGRESS: at 20.44% examples, 231561 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:13,844 : INFO : EPOCH 1 - PROGRESS: at 43.35% examples, 235834 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:14,869 : INFO : EPOCH 1 - PROGRESS: at 65.64% examples, 242884 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:15,896 : INFO : EPOCH 1 - PROGRESS: at 90.77% examples, 245388 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:16,204 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 16:52:16,226 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 16:52:16,232 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 16:52:16,232 : INFO : EPOCH - 1 : training on 1225961 raw words (1091765 effective words) took 4.4s, 247948 effective words/s\n",
      "2019-06-26 16:52:17,243 : INFO : EPOCH 2 - PROGRESS: at 21.29% examples, 239026 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:18,288 : INFO : EPOCH 2 - PROGRESS: at 45.82% examples, 244065 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:19,328 : INFO : EPOCH 2 - PROGRESS: at 68.47% examples, 247302 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:20,355 : INFO : EPOCH 2 - PROGRESS: at 93.23% examples, 248637 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:20,561 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 16:52:20,570 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 16:52:20,575 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 16:52:20,575 : INFO : EPOCH - 2 : training on 1225961 raw words (1091662 effective words) took 4.3s, 251537 effective words/s\n",
      "2019-06-26 16:52:21,614 : INFO : EPOCH 3 - PROGRESS: at 22.28% examples, 241032 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:22,635 : INFO : EPOCH 3 - PROGRESS: at 45.82% examples, 243551 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:23,713 : INFO : EPOCH 3 - PROGRESS: at 64.69% examples, 232716 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:24,746 : INFO : EPOCH 3 - PROGRESS: at 85.50% examples, 224766 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:25,457 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 16:52:25,467 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 16:52:25,480 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 16:52:25,480 : INFO : EPOCH - 3 : training on 1225961 raw words (1091637 effective words) took 4.9s, 222687 effective words/s\n",
      "2019-06-26 16:52:26,545 : INFO : EPOCH 4 - PROGRESS: at 21.29% examples, 226972 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:27,617 : INFO : EPOCH 4 - PROGRESS: at 45.82% examples, 234760 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:28,620 : INFO : EPOCH 4 - PROGRESS: at 66.62% examples, 238160 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:29,659 : INFO : EPOCH 4 - PROGRESS: at 90.77% examples, 238995 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:29,972 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 16:52:29,977 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 16:52:29,993 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 16:52:29,993 : INFO : EPOCH - 4 : training on 1225961 raw words (1091559 effective words) took 4.5s, 242036 effective words/s\n",
      "2019-06-26 16:52:31,026 : INFO : EPOCH 5 - PROGRESS: at 21.29% examples, 233894 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:32,102 : INFO : EPOCH 5 - PROGRESS: at 40.56% examples, 213845 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:33,145 : INFO : EPOCH 5 - PROGRESS: at 56.65% examples, 203321 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-26 16:52:34,165 : INFO : EPOCH 5 - PROGRESS: at 78.09% examples, 206658 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-26 16:52:35,108 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 16:52:35,111 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 16:52:35,125 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 16:52:35,126 : INFO : EPOCH - 5 : training on 1225961 raw words (1091662 effective words) took 5.1s, 212833 effective words/s\n",
      "2019-06-26 16:52:35,126 : INFO : training on a 6129805 raw words (5458285 effective words) took 23.3s, 234272 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5458285, 6129805)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(processed_docs, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.wv['paris']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5680741"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.wv.similarity('paris','france')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-26 16:25:15,611 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-26 16:25:16,563 : INFO : adding document #10000 to Dictionary(95752 unique tokens: ['accord', 'activities', 'actor', 'agency', 'agenda']...)\n",
      "2019-06-26 16:25:16,574 : INFO : built Dictionary(96426 unique tokens: ['accord', 'activities', 'actor', 'agency', 'agenda']...) from 10102 documents (total 1225961 corpus positions)\n",
      "2019-06-26 16:25:16,583 : INFO : saving Dictionary object under data/wiki_set1.dict, separately None\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-06-26 16:25:16,633 : INFO : saved data/wiki_set1.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(96426 unique tokens: ['accord', 'activities', 'actor', 'agency', 'agenda']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "dictionary.save('data/wiki_set1.dict')  # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accord': 0,\n",
       " 'activities': 1,\n",
       " 'actor': 2,\n",
       " 'agency': 3,\n",
       " 'agenda': 4,\n",
       " 'agent': 5,\n",
       " 'agents': 6,\n",
       " 'aide': 7,\n",
       " 'alch': 8,\n",
       " 'alcoholic': 9,\n",
       " 'alexander': 10,\n",
       " 'alger': 11,\n",
       " 'alumni': 12,\n",
       " 'america': 13,\n",
       " 'american': 14,\n",
       " 'anderson': 15,\n",
       " 'archive': 16,\n",
       " 'assassinations': 17,\n",
       " 'associate': 18,\n",
       " 'atomic': 19,\n",
       " 'attack': 20,\n",
       " 'attorney': 21,\n",
       " 'author': 22,\n",
       " 'background': 23,\n",
       " 'bedell': 24,\n",
       " 'bernard': 25,\n",
       " 'book': 26,\n",
       " 'borrow': 27,\n",
       " 'break': 28,\n",
       " 'bring': 29,\n",
       " 'bureau': 30,\n",
       " 'bush': 31,\n",
       " 'call': 32,\n",
       " 'campaign': 33,\n",
       " 'career': 34,\n",
       " 'case': 35,\n",
       " 'cash': 36,\n",
       " 'catholic': 37,\n",
       " 'chair': 38,\n",
       " 'chairman': 39,\n",
       " 'chamber': 40,\n",
       " 'characterize': 41,\n",
       " 'check': 42,\n",
       " 'circa': 43,\n",
       " 'circumstances': 44,\n",
       " 'clients': 45,\n",
       " 'committee': 46,\n",
       " 'communist': 47,\n",
       " 'company': 48,\n",
       " 'conference': 49,\n",
       " 'congressional': 50,\n",
       " 'contract': 51,\n",
       " 'credibility': 52,\n",
       " 'crises': 53,\n",
       " 'ctia': 54,\n",
       " 'death': 55,\n",
       " 'deaths': 56,\n",
       " 'decade': 57,\n",
       " 'deep': 58,\n",
       " 'detective': 59,\n",
       " 'die': 60,\n",
       " 'director': 61,\n",
       " 'discuss': 62,\n",
       " 'dismiss': 63,\n",
       " 'dozen': 64,\n",
       " 'edgar': 65,\n",
       " 'edward': 66,\n",
       " 'eisler': 67,\n",
       " 'enable': 68,\n",
       " 'evidence': 69,\n",
       " 'express': 70,\n",
       " 'external': 71,\n",
       " 'federal': 72,\n",
       " 'fensterwald': 73,\n",
       " 'frank': 74,\n",
       " 'general': 75,\n",
       " 'george': 76,\n",
       " 'gerald': 77,\n",
       " 'gerhart': 78,\n",
       " 'give': 79,\n",
       " 'graduate': 80,\n",
       " 'guard': 81,\n",
       " 'guide': 82,\n",
       " 'harold': 83,\n",
       " 'heart': 84,\n",
       " 'help': 85,\n",
       " 'herbert': 86,\n",
       " 'hire': 87,\n",
       " 'hiss': 88,\n",
       " 'hollywood': 89,\n",
       " 'hoover': 90,\n",
       " 'hougan': 91,\n",
       " 'house': 92,\n",
       " 'howard': 93,\n",
       " 'huac': 94,\n",
       " 'include': 95,\n",
       " 'industry': 96,\n",
       " 'influence': 97,\n",
       " 'information': 98,\n",
       " 'international': 99,\n",
       " 'investigate': 100,\n",
       " 'investigation': 101,\n",
       " 'investigative': 102,\n",
       " 'investigator': 103,\n",
       " 'jack': 104,\n",
       " 'jam': 105,\n",
       " 'january': 106,\n",
       " 'job': 107,\n",
       " 'johnson': 108,\n",
       " 'join': 109,\n",
       " 'josephson': 110,\n",
       " 'july': 111,\n",
       " 'june': 112,\n",
       " 'know': 113,\n",
       " 'koral': 114,\n",
       " 'lead': 115,\n",
       " 'leak': 116,\n",
       " 'legacy': 117,\n",
       " 'leon': 118,\n",
       " 'library': 119,\n",
       " 'life': 120,\n",
       " 'link': 121,\n",
       " 'louis': 122,\n",
       " 'manager': 123,\n",
       " 'manhattan': 124,\n",
       " 'materials': 125,\n",
       " 'mccord': 126,\n",
       " 'memoir': 127,\n",
       " 'merry': 128,\n",
       " 'motel': 129,\n",
       " 'motion': 130,\n",
       " 'national': 131,\n",
       " 'nixon': 132,\n",
       " 'obtain': 133,\n",
       " 'office': 134,\n",
       " 'offices': 135,\n",
       " 'originally': 136,\n",
       " 'paper': 137,\n",
       " 'people': 138,\n",
       " 'personal': 139,\n",
       " 'picture': 140,\n",
       " 'prepare': 141,\n",
       " 'press': 142,\n",
       " 'private': 143,\n",
       " 'project': 144,\n",
       " 'provide': 145,\n",
       " 'recall': 146,\n",
       " 'recount': 147,\n",
       " 'reference': 148,\n",
       " 'refuse': 149,\n",
       " 'rehired': 150,\n",
       " 'remain': 151,\n",
       " 'report': 152,\n",
       " 'representative': 153,\n",
       " 'republican': 154,\n",
       " 'restrain': 155,\n",
       " 'return': 156,\n",
       " 'richard': 157,\n",
       " 'ring': 158,\n",
       " 'robert': 159,\n",
       " 'robinson': 160,\n",
       " 'round': 161,\n",
       " 'russell': 162,\n",
       " 'secret': 163,\n",
       " 'secrets': 164,\n",
       " 'security': 165,\n",
       " 'send': 166,\n",
       " 'senior': 167,\n",
       " 'serve': 168,\n",
       " 'service': 169,\n",
       " 'smith': 170,\n",
       " 'soviet': 171,\n",
       " 'start': 172,\n",
       " 'street': 173,\n",
       " 'strike': 174,\n",
       " 'stripling': 175,\n",
       " 'successor': 176,\n",
       " 'sullivan': 177,\n",
       " 'tavenner': 178,\n",
       " 'testify': 179,\n",
       " 'testimony': 180,\n",
       " 'throat': 181,\n",
       " 'time': 182,\n",
       " 'unable': 183,\n",
       " 'uncover': 184,\n",
       " 'undermine': 185,\n",
       " 'union': 186,\n",
       " 'university': 187,\n",
       " 'velde': 188,\n",
       " 'walker': 189,\n",
       " 'walter': 190,\n",
       " 'washington': 191,\n",
       " 'watergate': 192,\n",
       " 'whittaker': 193,\n",
       " 'william': 194,\n",
       " 'womanizer': 195,\n",
       " 'work': 196,\n",
       " 'write': 197,\n",
       " 'writer': 198,\n",
       " 'account': 199,\n",
       " 'activists': 200,\n",
       " 'adult': 201,\n",
       " 'advancement': 202,\n",
       " 'advocate': 203,\n",
       " 'allen': 204,\n",
       " 'allow': 205,\n",
       " 'almshouse': 206,\n",
       " 'anne': 207,\n",
       " 'anthony': 208,\n",
       " 'arm': 209,\n",
       " 'arrange': 210,\n",
       " 'assistance': 211,\n",
       " 'association': 212,\n",
       " 'associations': 213,\n",
       " 'attribution': 214,\n",
       " 'bear': 215,\n",
       " 'bibliography': 216,\n",
       " 'birth': 217,\n",
       " 'body': 218,\n",
       " 'cabinet': 219,\n",
       " 'cady': 220,\n",
       " 'cause': 221,\n",
       " 'century': 222,\n",
       " 'certain': 223,\n",
       " 'childhood': 224,\n",
       " 'children': 225,\n",
       " 'christian': 226,\n",
       " 'church': 227,\n",
       " 'citations': 228,\n",
       " 'class': 229,\n",
       " 'college': 230,\n",
       " 'condition': 231,\n",
       " 'confirm': 232,\n",
       " 'congestion': 233,\n",
       " 'constituents': 234,\n",
       " 'contribute': 235,\n",
       " 'convention': 236,\n",
       " 'county': 237,\n",
       " 'crawfordsville': 238,\n",
       " 'daughters': 239,\n",
       " 'discoveries': 240,\n",
       " 'diseases': 241,\n",
       " 'early': 242,\n",
       " 'edit': 243,\n",
       " 'education': 244,\n",
       " 'effect': 245,\n",
       " 'eleazer': 246,\n",
       " 'elizabeth': 247,\n",
       " 'employ': 248,\n",
       " 'employment': 249,\n",
       " 'engage': 250,\n",
       " 'enter': 251,\n",
       " 'equal': 252,\n",
       " 'especially': 253,\n",
       " 'establish': 254,\n",
       " 'expression': 255,\n",
       " 'father': 256,\n",
       " 'february': 257,\n",
       " 'female': 258,\n",
       " 'fiction': 259,\n",
       " 'financial': 260,\n",
       " 'founder': 261,\n",
       " 'fund': 262,\n",
       " 'gain': 263,\n",
       " 'girls': 264,\n",
       " 'greatest': 265,\n",
       " 'holloway': 266,\n",
       " 'home': 267,\n",
       " 'husband': 268,\n",
       " 'important': 269,\n",
       " 'indiana': 270,\n",
       " 'infancy': 271,\n",
       " 'interest': 272,\n",
       " 'kentucky': 273,\n",
       " 'king': 274,\n",
       " 'leader': 275,\n",
       " 'livermore': 276,\n",
       " 'local': 277,\n",
       " 'lungs': 278,\n",
       " 'maker': 279,\n",
       " 'marry': 280,\n",
       " 'mary': 281,\n",
       " 'mcdowell': 282,\n",
       " 'medical': 283,\n",
       " 'medicine': 284,\n",
       " 'meet': 285,\n",
       " 'membership': 286,\n",
       " 'mitchell': 287,\n",
       " 'montgomery': 288,\n",
       " 'mother': 289,\n",
       " 'name': 290,\n",
       " 'native': 291,\n",
       " 'nature': 292,\n",
       " 'near': 293,\n",
       " 'newspapers': 294,\n",
       " 'nonetheless': 295,\n",
       " 'open': 296,\n",
       " 'opposition': 297,\n",
       " 'organic': 298,\n",
       " 'organize': 299,\n",
       " 'orphan': 300,\n",
       " 'patient': 301,\n",
       " 'paupers': 302,\n",
       " 'pennsylvania': 303,\n",
       " 'pharmaceuticals': 304,\n",
       " 'philadelphia': 305,\n",
       " 'philanthropist': 306,\n",
       " 'philanthropists': 307,\n",
       " 'physician': 308,\n",
       " 'physicians': 309,\n",
       " 'pioneer': 310,\n",
       " 'pneumonia': 311,\n",
       " 'poems': 312,\n",
       " 'poetic': 313,\n",
       " 'popularity': 314,\n",
       " 'practice': 315,\n",
       " 'practitioner': 316,\n",
       " 'president': 317,\n",
       " 'publish': 318,\n",
       " 'regularly': 319,\n",
       " 'representatives': 320,\n",
       " 'right': 321,\n",
       " 'save': 322,\n",
       " 'school': 323,\n",
       " 'secretary': 324,\n",
       " 'self': 325,\n",
       " 'sell': 326,\n",
       " 'sergeant': 327,\n",
       " 'seventeen': 328,\n",
       " 'sew': 329,\n",
       " 'short': 330,\n",
       " 'sons': 331,\n",
       " 'speakers': 332,\n",
       " 'stanton': 333,\n",
       " 'state': 334,\n",
       " 'subject': 335,\n",
       " 'subscriptions': 336,\n",
       " 'subsequently': 337,\n",
       " 'success': 338,\n",
       " 'suffrage': 339,\n",
       " 'suffragists': 340,\n",
       " 'support': 341,\n",
       " 'survive': 342,\n",
       " 'susan': 343,\n",
       " 'tailor': 344,\n",
       " 'teach': 345,\n",
       " 'teenager': 346,\n",
       " 'thesis': 347,\n",
       " 'tobacco': 348,\n",
       " 'treatment': 349,\n",
       " 'unceasing': 350,\n",
       " 'unhappy': 351,\n",
       " 'unite': 352,\n",
       " 'verse': 353,\n",
       " 'vice': 354,\n",
       " 'virginia': 355,\n",
       " 'welfare': 356,\n",
       " 'whisky': 357,\n",
       " 'wilcox': 358,\n",
       " 'wilhite': 359,\n",
       " 'wish': 360,\n",
       " 'wixon': 361,\n",
       " 'woman': 362,\n",
       " 'women': 363,\n",
       " 'writers': 364,\n",
       " 'years': 365,\n",
       " 'young': 366,\n",
       " 'african': 367,\n",
       " 'ambassador': 368,\n",
       " 'army': 369,\n",
       " 'bangui': 370,\n",
       " 'bilateral': 371,\n",
       " 'border': 372,\n",
       " 'cemac': 373,\n",
       " 'central': 374,\n",
       " 'civil': 375,\n",
       " 'conflict': 376,\n",
       " 'congo': 377,\n",
       " 'congolese': 378,\n",
       " 'cooperate': 379,\n",
       " 'cooperation': 380,\n",
       " 'countries': 381,\n",
       " 'country': 382,\n",
       " 'coup': 383,\n",
       " 'cross': 384,\n",
       " 'current': 385,\n",
       " 'deal': 386,\n",
       " 'democratic': 387,\n",
       " 'embassy': 388,\n",
       " 'fardc': 389,\n",
       " 'force': 390,\n",
       " 'gaspard': 391,\n",
       " 'governments': 392,\n",
       " 'historic': 393,\n",
       " 'insurgency': 394,\n",
       " 'long': 395,\n",
       " 'lord': 396,\n",
       " 'military': 397,\n",
       " 'mission': 398,\n",
       " 'mugaruka': 399,\n",
       " 'nationals': 400,\n",
       " 'neighbour': 401,\n",
       " 'officer': 402,\n",
       " 'ongoing': 403,\n",
       " 'peacekeepers': 404,\n",
       " 'peacekeeping': 405,\n",
       " 'police': 406,\n",
       " 'refer': 407,\n",
       " 'refugees': 408,\n",
       " 'relations': 409,\n",
       " 'relationship': 410,\n",
       " 'republic': 411,\n",
       " 'resistance': 412,\n",
       " 'share': 413,\n",
       " 'side': 414,\n",
       " 'soldier': 415,\n",
       " 'territory': 416,\n",
       " 'troop': 417,\n",
       " 'état': 418,\n",
       " 'adapt': 419,\n",
       " 'appear': 420,\n",
       " 'april': 421,\n",
       " 'attend': 422,\n",
       " 'august': 423,\n",
       " 'continue': 424,\n",
       " 'couple': 425,\n",
       " 'cultural': 426,\n",
       " 'czech': 427,\n",
       " 'daughter': 428,\n",
       " 'editor': 429,\n",
       " 'french': 430,\n",
       " 'germain': 431,\n",
       " 'go': 432,\n",
       " 'grammar': 433,\n",
       " 'grandfather': 434,\n",
       " 'hanácký': 435,\n",
       " 'herma': 436,\n",
       " 'hlasu': 437,\n",
       " 'jablonecka': 438,\n",
       " 'johnová': 439,\n",
       " 'journal': 440,\n",
       " 'journalism': 441,\n",
       " 'kalendář': 442,\n",
       " 'kyrat': 443,\n",
       " 'laye': 444,\n",
       " 'lidové': 445,\n",
       " 'lidu': 446,\n",
       " 'lyceum': 447,\n",
       " 'marriage': 448,\n",
       " 'mayor': 449,\n",
       " 'move': 450,\n",
       " 'napětí': 451,\n",
       " 'ndash': 452,\n",
       " 'novelists': 453,\n",
       " 'novels': 454,\n",
       " 'noviny': 455,\n",
       " 'oxford': 456,\n",
       " 'paternal': 457,\n",
       " 'periodicals': 458,\n",
       " 'play': 459,\n",
       " 'poets': 460,\n",
       " 'prague': 461,\n",
       " 'prose': 462,\n",
       " 'prostějov': 463,\n",
       " 'radio': 464,\n",
       " 'saint': 465,\n",
       " 'study': 466,\n",
       " 'svozilová': 467,\n",
       " 'vysoké': 468,\n",
       " 'anthologies': 469,\n",
       " 'aren': 470,\n",
       " 'bachelor': 471,\n",
       " 'british': 472,\n",
       " 'burnaby': 473,\n",
       " 'canadian': 474,\n",
       " 'cariboo': 475,\n",
       " 'celebrate': 476,\n",
       " 'center': 477,\n",
       " 'columbia': 478,\n",
       " 'comprise': 479,\n",
       " 'corpse': 480,\n",
       " 'degrees': 481,\n",
       " 'department': 482,\n",
       " 'dragonhunt': 483,\n",
       " 'drop': 484,\n",
       " 'duncan': 485,\n",
       " 'egoff': 486,\n",
       " 'ferris': 487,\n",
       " 'finalists': 488,\n",
       " 'find': 489,\n",
       " 'follow': 490,\n",
       " 'frances': 491,\n",
       " 'gabriola': 492,\n",
       " 'genie': 493,\n",
       " 'gold': 494,\n",
       " 'grace': 495,\n",
       " 'health': 496,\n",
       " 'hold': 497,\n",
       " 'investigations': 498,\n",
       " 'island': 499,\n",
       " 'islands': 500,\n",
       " 'journey': 501,\n",
       " 'juan': 502,\n",
       " 'kelly': 503,\n",
       " 'kiss': 504,\n",
       " 'land': 505,\n",
       " 'listen': 506,\n",
       " 'literature': 507,\n",
       " 'live': 508,\n",
       " 'love': 509,\n",
       " 'magazines': 510,\n",
       " 'makara': 511,\n",
       " 'makers': 512,\n",
       " 'master': 513,\n",
       " 'mental': 514,\n",
       " 'metropolitan': 515,\n",
       " 'mineral': 516,\n",
       " 'mugger': 517,\n",
       " 'mysteries': 518,\n",
       " 'northern': 519,\n",
       " 'novel': 520,\n",
       " 'pattern': 521,\n",
       " 'prize': 522,\n",
       " 'psychologist': 523,\n",
       " 'quadra': 524,\n",
       " 'resources': 525,\n",
       " 'runaway': 526,\n",
       " 'rush': 527,\n",
       " 'sandy': 528,\n",
       " 'sheila': 529,\n",
       " 'sing': 530,\n",
       " 'sleep': 531,\n",
       " 'stories': 532,\n",
       " 'story': 533,\n",
       " 'suspect': 534,\n",
       " 'szanto': 535,\n",
       " 'tell': 536,\n",
       " 'thread': 537,\n",
       " 'toothpaste': 538,\n",
       " 'turn': 539,\n",
       " 'vancouver': 540,\n",
       " 'villain': 541,\n",
       " 'water': 542,\n",
       " 'westminster': 543,\n",
       " 'whidbey': 544,\n",
       " 'woodlands': 545,\n",
       " 'apply': 546,\n",
       " 'aptitude': 547,\n",
       " 'armstrong': 548,\n",
       " 'australian': 549,\n",
       " 'award': 550,\n",
       " 'basic': 551,\n",
       " 'blackboard': 552,\n",
       " 'brazil': 553,\n",
       " 'cedric': 554,\n",
       " 'centre': 555,\n",
       " 'clarity': 556,\n",
       " 'complete': 557,\n",
       " 'course': 558,\n",
       " 'degree': 559,\n",
       " 'demand': 560,\n",
       " 'diploma': 561,\n",
       " 'doctorate': 562,\n",
       " 'economics': 563,\n",
       " 'france': 564,\n",
       " 'fundação': 565,\n",
       " 'geologists': 566,\n",
       " 'georges': 567,\n",
       " 'geosciences': 568,\n",
       " 'geoscientist': 569,\n",
       " 'geostatistical': 570,\n",
       " 'geostatistician': 571,\n",
       " 'geostatistics': 572,\n",
       " 'getúlio': 573,\n",
       " 'great': 574,\n",
       " 'griffiths': 575,\n",
       " 'industrial': 576,\n",
       " 'john': 577,\n",
       " 'linear': 578,\n",
       " 'margaret': 579,\n",
       " 'mathematical': 580,\n",
       " 'mathematics': 581,\n",
       " 'matheron': 582,\n",
       " 'mine': 583,\n",
       " 'miss': 584,\n",
       " 'note': 585,\n",
       " 'paris': 586,\n",
       " 'paristech': 587,\n",
       " 'plurigaussian': 588,\n",
       " 'professor': 589,\n",
       " 'queensland': 590,\n",
       " 'recognition': 591,\n",
       " 'research': 592,\n",
       " 'scientists': 593,\n",
       " 'simulations': 594,\n",
       " 'springer': 595,\n",
       " 'statement': 596,\n",
       " 'statisticians': 597,\n",
       " 'teacher': 598,\n",
       " 'textbook': 599,\n",
       " 'vargas': 600,\n",
       " 'winner': 601,\n",
       " 'year': 602,\n",
       " 'école': 603,\n",
       " 'accurate': 604,\n",
       " 'act': 605,\n",
       " 'adjust': 606,\n",
       " 'algorithm': 607,\n",
       " 'algorithms': 608,\n",
       " 'ancilla': 609,\n",
       " 'approximate': 610,\n",
       " 'attempt': 611,\n",
       " 'bigger': 612,\n",
       " 'black': 613,\n",
       " 'change': 614,\n",
       " 'circuit': 615,\n",
       " 'color': 616,\n",
       " 'come': 617,\n",
       " 'complexities': 618,\n",
       " 'complexity': 619,\n",
       " 'computational': 620,\n",
       " 'count': 621,\n",
       " 'decompose': 622,\n",
       " 'decompositions': 623,\n",
       " 'depend': 624,\n",
       " 'describe': 625,\n",
       " 'distribute': 626,\n",
       " 'edge': 627,\n",
       " 'efficiently': 628,\n",
       " 'eigenvalues': 629,\n",
       " 'entry': 630,\n",
       " 'epsilon': 631,\n",
       " 'error': 632,\n",
       " 'estimation': 633,\n",
       " 'evolution': 634,\n",
       " 'expansion': 635,\n",
       " 'explicitly': 636,\n",
       " 'exponentially': 637,\n",
       " 'feynman': 638,\n",
       " 'finally': 639,\n",
       " 'formulas': 640,\n",
       " 'frac': 641,\n",
       " 'gate': 642,\n",
       " 'goal': 643,\n",
       " 'grow': 644,\n",
       " 'hamiltonian': 645,\n",
       " 'hamiltonians': 646,\n",
       " 'hamiltonin': 647,\n",
       " 'ideal': 648,\n",
       " 'identity': 649,\n",
       " 'implement': 650,\n",
       " 'implementations': 651,\n",
       " 'infty': 652,\n",
       " 'instead': 653,\n",
       " 'large': 654,\n",
       " 'largest': 655,\n",
       " 'like': 656,\n",
       " 'make': 657,\n",
       " 'mathop': 658,\n",
       " 'matrix': 659,\n",
       " 'matter': 660,\n",
       " 'maximum': 661,\n",
       " 'mention': 662,\n",
       " 'methods': 663,\n",
       " 'need': 664,\n",
       " 'number': 665,\n",
       " 'occur': 666,\n",
       " 'operation': 667,\n",
       " 'optimal': 668,\n",
       " 'oracle': 669,\n",
       " 'phase': 670,\n",
       " 'possible': 671,\n",
       " 'practical': 672,\n",
       " 'previously': 673,\n",
       " 'problem': 674,\n",
       " 'process': 675,\n",
       " 'product': 676,\n",
       " 'propose': 677,\n",
       " 'prove': 678,\n",
       " 'quantum': 679,\n",
       " 'qubit': 680,\n",
       " 'qubits': 681,\n",
       " 'query': 682,\n",
       " 'relate': 683,\n",
       " 'repetitions': 684,\n",
       " 'represent': 685,\n",
       " 'respect': 686,\n",
       " 'rotations': 687,\n",
       " 'say': 688,\n",
       " 'science': 689,\n",
       " 'second': 690,\n",
       " 'separately': 691,\n",
       " 'series': 692,\n",
       " 'show': 693,\n",
       " 'signal': 694,\n",
       " 'simulate': 695,\n",
       " 'simulation': 696,\n",
       " 'single': 697,\n",
       " 'size': 698,\n",
       " 'slice': 699,\n",
       " 'small': 700,\n",
       " 'solution': 701,\n",
       " 'sparse': 702,\n",
       " 'special': 703,\n",
       " 'spectrum': 704,\n",
       " 'step': 705,\n",
       " 'sum_': 706,\n",
       " 'suzuki': 707,\n",
       " 'systems': 708,\n",
       " 'table': 709,\n",
       " 'taylor': 710,\n",
       " 'techniques': 711,\n",
       " 'term': 712,\n",
       " 'transduce': 713,\n",
       " 'transform': 714,\n",
       " 'trivially': 715,\n",
       " 'trotter': 716,\n",
       " 'truncate': 717,\n",
       " 'twice': 718,\n",
       " 'unitary': 719,\n",
       " 'unnecessary': 720,\n",
       " 'walk': 721,\n",
       " 'ways': 722,\n",
       " 'abolish': 723,\n",
       " 'additional': 724,\n",
       " 'aldermen': 725,\n",
       " 'available': 726,\n",
       " 'board': 727,\n",
       " 'borough': 728,\n",
       " 'bronx': 729,\n",
       " 'brooklyn': 730,\n",
       " 'cashmore': 731,\n",
       " 'cast': 732,\n",
       " 'city': 733,\n",
       " 'communists': 734,\n",
       " 'consequently': 735,\n",
       " 'contrast': 736,\n",
       " 'council': 737,\n",
       " 'december': 738,\n",
       " 'democrats': 739,\n",
       " 'district': 740,\n",
       " 'elect': 741,\n",
       " 'election': 742,\n",
       " 'elections': 743,\n",
       " 'entitle': 744,\n",
       " 'expect': 745,\n",
       " 'form': 746,\n",
       " 'fusion': 747,\n",
       " 'greater': 748,\n",
       " 'labor': 749,\n",
       " 'later': 750,\n",
       " 'majorities': 751,\n",
       " 'majority': 752,\n",
       " 'member': 753,\n",
       " 'members': 754,\n",
       " 'month': 755,\n",
       " 'novelty': 756,\n",
       " 'november': 757,\n",
       " 'owe': 758,\n",
       " 'party': 759,\n",
       " 'proportional': 760,\n",
       " 'queen': 761,\n",
       " 'receive': 762,\n",
       " 'remainder': 763,\n",
       " 'replace': 764,\n",
       " 'representation': 765,\n",
       " 'response': 766,\n",
       " 'result': 767,\n",
       " 'richmond': 768,\n",
       " 'seven': 769,\n",
       " 'significantly': 770,\n",
       " 'slow': 771,\n",
       " 'vote': 772,\n",
       " 'york': 773,\n",
       " 'adeniji': 774,\n",
       " 'adrian': 775,\n",
       " 'bank': 776,\n",
       " 'barcome': 777,\n",
       " 'bird': 778,\n",
       " 'boorman': 779,\n",
       " 'brett': 780,\n",
       " 'bridge': 781,\n",
       " 'brien': 782,\n",
       " 'burman': 783,\n",
       " 'carr': 784,\n",
       " 'castle': 785,\n",
       " 'chailey': 786,\n",
       " 'chiltington': 787,\n",
       " 'chris': 788,\n",
       " 'christine': 789,\n",
       " 'christoph': 790,\n",
       " 'clay': 791,\n",
       " 'cliffs': 792,\n",
       " 'collier': 793,\n",
       " 'compose': 794,\n",
       " 'connor': 795,\n",
       " 'conservative': 796,\n",
       " 'conservatives': 797,\n",
       " 'control': 798,\n",
       " 'councillors': 799,\n",
       " 'davey': 800,\n",
       " 'david': 801,\n",
       " 'davis': 802,\n",
       " 'dawn': 803,\n",
       " 'democrat': 804,\n",
       " 'denis': 805,\n",
       " 'detail': 806,\n",
       " 'ditchling': 807,\n",
       " 'duhigg': 808,\n",
       " 'east': 809,\n",
       " 'emily': 810,\n",
       " 'england': 811,\n",
       " 'english': 812,\n",
       " 'gauntlett': 813,\n",
       " 'geoff': 814,\n",
       " 'graham': 815,\n",
       " 'green': 816,\n",
       " 'hamsey': 817,\n",
       " 'imogen': 818,\n",
       " 'independent': 819,\n",
       " 'independents': 820,\n",
       " 'isabelle': 821,\n",
       " 'johnny': 822,\n",
       " 'jones': 823,\n",
       " 'julian': 824,\n",
       " 'julie': 825,\n",
       " 'keefe': 826,\n",
       " 'keira': 827,\n",
       " 'kingston': 828,\n",
       " 'kurthy': 829,\n",
       " 'labour': 830,\n",
       " 'laurence': 831,\n",
       " 'lewes': 832,\n",
       " 'liberal': 833,\n",
       " 'linington': 834,\n",
       " 'lynda': 835,\n",
       " 'macleod': 836,\n",
       " 'makepeace': 837,\n",
       " 'manley': 838,\n",
       " 'maskell': 839,\n",
       " 'matthew': 840,\n",
       " 'mccleary': 841,\n",
       " 'meyer': 842,\n",
       " 'miller': 843,\n",
       " 'milly': 844,\n",
       " 'newhaven': 845,\n",
       " 'newick': 846,\n",
       " 'nicholson': 847,\n",
       " 'nicola': 848,\n",
       " 'north': 849,\n",
       " 'ouse': 850,\n",
       " 'overall': 851,\n",
       " 'papanicolaou': 852,\n",
       " 'peacehaven': 853,\n",
       " 'peterson': 854,\n",
       " 'phil': 855,\n",
       " 'place': 856,\n",
       " 'plumpton': 857,\n",
       " 'priory': 858,\n",
       " 'rigden': 859,\n",
       " 'ringmer': 860,\n",
       " 'rodger': 861,\n",
       " 'ross': 862,\n",
       " 'ruth': 863,\n",
       " 'rutland': 864,\n",
       " 'saltdean': 865,\n",
       " 'sandra': 866,\n",
       " 'saunders': 867,\n",
       " 'seaford': 868,\n",
       " 'sean': 869,\n",
       " 'sharon': 870,\n",
       " 'south': 871,\n",
       " 'stephen': 872,\n",
       " 'steve': 873,\n",
       " 'streat': 874,\n",
       " 'sussex': 875,\n",
       " 'sylvia': 876,\n",
       " 'take': 877,\n",
       " 'telscome': 878,\n",
       " 'town': 879,\n",
       " 'valley': 880,\n",
       " 'villages': 881,\n",
       " 'wales': 882,\n",
       " 'ward': 883,\n",
       " 'west': 884,\n",
       " 'westmeston': 885,\n",
       " 'white': 886,\n",
       " 'dutch': 887,\n",
       " 'football': 888,\n",
       " 'game': 889,\n",
       " 'legend': 890,\n",
       " 'list': 891,\n",
       " 'netherlands': 892,\n",
       " 'team': 893,\n",
       " 'australia': 894,\n",
       " 'band': 895,\n",
       " 'beatty': 896,\n",
       " 'coleman': 897,\n",
       " 'comedy': 898,\n",
       " 'dabney': 899,\n",
       " 'drama': 900,\n",
       " 'film': 901,\n",
       " 'indie': 902,\n",
       " 'melbourne': 903,\n",
       " 'music': 904,\n",
       " 'pray': 905,\n",
       " 'ritter': 906,\n",
       " 'rock': 907,\n",
       " 'spoof': 908,\n",
       " 'star': 909,\n",
       " 'televangelism': 910,\n",
       " 'television': 911,\n",
       " 'abang': 912,\n",
       " 'abdillah': 913,\n",
       " 'active': 914,\n",
       " 'anak': 915,\n",
       " 'anwar': 916,\n",
       " 'appoint': 917,\n",
       " 'assembly': 918,\n",
       " 'betong': 919,\n",
       " 'biju': 920,\n",
       " 'constituency': 921,\n",
       " 'currently': 922,\n",
       " 'datu': 923,\n",
       " 'dewan': 924,\n",
       " 'division': 925,\n",
       " 'engineer': 926,\n",
       " 'haji': 927,\n",
       " 'high': 928,\n",
       " 'ibrahim': 929,\n",
       " 'justice': 930,\n",
       " 'krian': 931,\n",
       " 'leadership': 932,\n",
       " 'legislative': 933,\n",
       " 'malaysia': 934,\n",
       " 'malaysian': 935,\n",
       " 'parliament': 936,\n",
       " 'parliamentary': 937,\n",
       " 'patinggi': 938,\n",
       " 'politician': 939,\n",
       " 'politicians': 940,\n",
       " 'presidents': 941,\n",
       " 'profession': 942,\n",
       " 'rakyat': 943,\n",
       " 'saratok': 944,\n",
       " 'sarawak': 945,\n",
       " 'select': 946,\n",
       " 'sit': 947,\n",
       " 'stub': 948,\n",
       " 'valparaiso': 949,\n",
       " 'ashley': 950,\n",
       " 'build': 951,\n",
       " 'camp': 952,\n",
       " 'carter': 953,\n",
       " 'criteria': 954,\n",
       " 'daggett': 955,\n",
       " 'defense': 956,\n",
       " 'domestic': 957,\n",
       " 'event': 958,\n",
       " 'facility': 959,\n",
       " 'forest': 960,\n",
       " 'function': 961,\n",
       " 'portion': 962,\n",
       " 'potential': 963,\n",
       " 'register': 964,\n",
       " 'road': 965,\n",
       " 'run': 966,\n",
       " 'structure': 967,\n",
       " 'subfunction': 968,\n",
       " 'trail': 969,\n",
       " 'transportation': 970,\n",
       " 'uintah': 971,\n",
       " 'utah': 972,\n",
       " 'wyoming': 973,\n",
       " 'begin': 974,\n",
       " 'congress': 975,\n",
       " 'convene': 976,\n",
       " 'tennessee': 977,\n",
       " 'advance': 978,\n",
       " 'affordable': 979,\n",
       " 'aion': 980,\n",
       " 'alongside': 981,\n",
       " 'auto': 982,\n",
       " 'battery': 983,\n",
       " 'blue': 984,\n",
       " 'brand': 985,\n",
       " 'catl': 986,\n",
       " 'chinese': 987,\n",
       " 'clear': 988,\n",
       " 'compact': 989,\n",
       " 'competitor': 990,\n",
       " 'dollars': 991,\n",
       " 'electric': 992,\n",
       " 'electronic': 993,\n",
       " 'emblem': 994,\n",
       " 'energy': 995,\n",
       " 'exterior': 996,\n",
       " 'feature': 997,\n",
       " 'group': 998,\n",
       " 'guangzhou': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-26 16:30:45,110 : INFO : storing corpus in Matrix Market format to data/wiki_set1.mm\n",
      "2019-06-26 16:30:45,117 : INFO : saving sparse matrix to data/wiki_set1.mm\n",
      "2019-06-26 16:30:45,118 : INFO : PROGRESS: saving document #0\n",
      "2019-06-26 16:30:45,238 : INFO : PROGRESS: saving document #1000\n",
      "2019-06-26 16:30:45,345 : INFO : PROGRESS: saving document #2000\n",
      "2019-06-26 16:30:45,442 : INFO : PROGRESS: saving document #3000\n",
      "2019-06-26 16:30:45,529 : INFO : PROGRESS: saving document #4000\n",
      "2019-06-26 16:30:45,623 : INFO : PROGRESS: saving document #5000\n",
      "2019-06-26 16:30:45,723 : INFO : PROGRESS: saving document #6000\n",
      "2019-06-26 16:30:45,809 : INFO : PROGRESS: saving document #7000\n",
      "2019-06-26 16:30:45,895 : INFO : PROGRESS: saving document #8000\n",
      "2019-06-26 16:30:45,985 : INFO : PROGRESS: saving document #9000\n",
      "2019-06-26 16:30:46,082 : INFO : PROGRESS: saving document #10000\n",
      "2019-06-26 16:30:46,092 : INFO : saved 10102x96426 matrix, density=0.075% (727212/974095452)\n",
      "2019-06-26 16:30:46,094 : INFO : saving MmCorpus index to data/wiki_set1.mm.index\n"
     ]
    }
   ],
   "source": [
    "corpora.MmCorpus.serialize('data/wiki_set1.mm', corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc1 = nlp(processed_docs[0])\n",
    "#doc2 = nlp(processed_docs[1])\n",
    "doc1 = nlp(long_text[0])\n",
    "doc2 = nlp(long_text[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9569170543269897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "print (doc1.similarity(doc2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def compare_docs(text):\n",
    "    all_scores =[]\n",
    "    nlp = spacy.load('en')\n",
    "    for i in range(len(long_text[0:10])):\n",
    "        doc1 = nlp(text)\n",
    "        doc2 = nlp(long_text[i])\n",
    "        score = doc1.similarity(doc2)\n",
    "        all_scores.append((i,score))\n",
    "    \n",
    "    scores_dict = dict(all_scores)\n",
    "    \n",
    "    return max(scores_dict.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 0.9421908017484784)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_docs(long_text[19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity = [textstat.flesch_kincaid_grade(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complexity = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complexity['score'] = pd.Series(complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complexity['text'] = pd.Series(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.6</td>\n",
       "      <td>Louis J. Russell (circa 1912–1973) was an agen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.1</td>\n",
       "      <td>Mary Holloway Wilhite (February 3, 1831 – Febr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                               text\n",
       "0   14.6  Louis J. Russell (circa 1912–1973) was an agen...\n",
       "1   11.1  Mary Holloway Wilhite (February 3, 1831 – Febr..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complexity.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10102.000000\n",
       "mean        15.854791\n",
       "std         16.623426\n",
       "min          1.900000\n",
       "25%         10.800000\n",
       "50%         13.200000\n",
       "75%         17.100000\n",
       "max       1046.200000\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complexity['score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting  \n",
    "four = []\n",
    "three = []\n",
    "two = []\n",
    "one = []\n",
    "for row in df_complexity['score']:\n",
    "    if row > 15: \n",
    "        four.append(row)\n",
    "    elif 15 >= row > 10:\n",
    "        three.append(row)\n",
    "    elif 10 < row <= 5:\n",
    "        two.append(row)\n",
    "    elif row <= 5:\n",
    "        one.append(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_one:43 , len_two:0 , len_three:4601 , len_four:3672\n"
     ]
    }
   ],
   "source": [
    "#Sorting \n",
    "print(\"len_one:\" + str(len(one)) + \" , len_two:\" + str(len(two)) \n",
    "      + \" , len_three:\" + str(len(three)) + \" , len_four:\" + str(len(four)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = [textstat.lexicon_count(doc, removepunct=True) for doc in docs] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complexity['num_words'] = pd.Series(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sent = [textstat.sentence_count(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complexity['num_sent'] = pd.Series(num_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fog = [textstat.gunning_fog(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_complexity['fog'] = pd.Series(fog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smog = [textstat.smog_index(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_complexity['smog'] = pd.Series(smog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_level = [textstat.automated_readability_index(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complexity['read_level'] = pd.Series(read_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c_liau = [textstat.coleman_liau_index(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_complexity['c_liau'] = pd.Series(c_liau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linsear = [textstat.linsear_write_formula(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_complexity['linsear'] = pd.Series(linsear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_chall = [textstat.dale_chall_readability_score(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_complexity['d_chall'] = pd.Series(d_chall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is takes into account all of the tests \n",
    "standard_score = [textstat.text_standard(doc, float_output=False) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complexity['standard_score'] = pd.Series(standard_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.6</td>\n",
       "      <td>Louis J. Russell (circa 1912–1973) was an agen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.1</td>\n",
       "      <td>Mary Holloway Wilhite (February 3, 1831 – Febr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.6</td>\n",
       "      <td>Central African Republic–Democratic Republic o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.4</td>\n",
       "      <td>Herma Svozilová-Johnová (August 29, 1914 &amp;ndas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.9</td>\n",
       "      <td>Sandy Frances Duncan is a Canadian writer of n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                               text\n",
       "0   14.6  Louis J. Russell (circa 1912–1973) was an agen...\n",
       "1   11.1  Mary Holloway Wilhite (February 3, 1831 – Febr...\n",
       "2   11.6  Central African Republic–Democratic Republic o...\n",
       "3    6.4  Herma Svozilová-Johnová (August 29, 1914 &ndas...\n",
       "4   18.9  Sandy Frances Duncan is a Canadian writer of n..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complexity.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Temnoye (links=no|Темное) is a rural locality (a village) in Chernushinsky District, Perm Krai, Russia. The population was 83 as of 2010.\\n== References ==\\nRural localities in Perm Krai'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textstat\n",
    "import operator\n",
    "\n",
    "def give_harder_level(text):\n",
    "    \"\"\"\n",
    "    Takes in text and returns an easier level read\n",
    "    \"\"\"\n",
    "    ##find all the articles that are easier than this article \n",
    "    all_easier_index = []\n",
    "    input_score = textstat.flesch_kincaid_grade(text)\n",
    "    for i in range(len(df_complexity['score'])):\n",
    "        if df_complexity['score'][i] > input_score:\n",
    "            all_easier_index.append(i)\n",
    "    \n",
    "    #return all_easier_index\n",
    "    \n",
    "    ##find the article most similar to this one\n",
    "    all_scores =[]\n",
    "    nlp = spacy.load('en')\n",
    "    for i in all_easier_index[0:30]:\n",
    "        doc1 = nlp(text)\n",
    "        doc2 = nlp(long_text[i])\n",
    "        score = doc1.similarity(doc2)\n",
    "        all_scores.append((i,score))\n",
    "    \n",
    "    scores_dict = dict(all_scores)\n",
    "    \n",
    "    best = max(scores_dict.items(), key=operator.itemgetter(1))\n",
    "    print(best)\n",
    "    index_best = list(best)[0]\n",
    "    return df_complexity['text'][index_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"A watermelon is a type of edible fruit, they are 92% water. About 6% of a watermelon is sugar, which makes it very sweet. There are many different types of watermelon. Some have a green rind on the outside and a red-pink flesh on the inside, with brown seeds. Some can have yellow flesh, and some can be seedless. The green rind on the outside is not usually eaten, though it can be used as a vegetable. It can also be stewed or pickled. Most watermelons are oblong or spherical. In Japan, watermelons are grown in different shapes. Many people like to eat watermelon in the summer because the fruit is cool and refreshing. Watermelons are a great source of vitamin A, vitamin C, vitamin B6 and vitamin B1. They also contain potassium, magnesium, carotenoid antioxidant, and lycopene. Watermelons are fruits that come from a vine-like plant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 0.8740465692490009)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hamiltonian simulation (also referred to as quantum simulation) is a problem in quantum information science that attempts to find the computational complexity and quantum algorithms needed for simulating quantum systems. Hamiltonian simulation is a problem that demands algorithms which implement the evolution of a quantum state efficiently. The Hamiltonian simulation problem was proposed by Richard Feynman in 1982, where he proposed a quantum computer as a possible solution since the simulation of general Hamiltonians seem to grow exponentially with respect to the system size. \\n== Problem statement ==\\nIn the Hamiltonian simulation problem, given a Hamiltonian H (2^n \\\\times 2^n [matrix] acting on  qubits, a time t and maximum simulation error \\\\epsilon, the goal is to find an algorithm that approximates  U  such that ||U - e^{-iht} || \\\\leq \\\\epsilon . where e^{-iht} is the ideal evolution.\\nA special case of the Hamiltonian simulation problem is the local Hamiltonian simulation problem. This is when H is a k-local Hamiltonian on n qubits where H = \\\\sum_{j \\\\mathop =1}^m H_j  and H_j acts non-trivially on at most k qubits instead of n qubits.  The local Hamiltonian simulation problem is important because most Hamiltonians that occur in nature are k-local. \\n== Techniques ==\\n=== Product Formulas ===\\nAlso known as Trotter formulas or Trotter-Suzuki decompositions, Product formulas simulate the sum-of-terms of a Hamiltonian by simulating each one separately for a small time slice.  If H = A + B + C , then  U = e^{-i(A + B + C)t} = (e^{-iAt/r}e^{-iBt/r}e^{-iCt/r})^r for a large r; where r is the number of time steps to simulate for. The large the r, the more accurate the simulation.\\nIf the Hamiltonian is represented as a Sparse matrix, the distributed edge coloring algorithm can be used to decompose it into a sum of terms; which can then be simulated by a Trotter-Suzuki algorithm. \\n=== Taylor Series ===\\n e^{-iHt} = \\\\sum_{n \\\\mathop = 0}^ \\\\infty \\\\frac{(-iHt)^{n}}{n} = I - iHt - \\\\frac{H^{2}t^{2}}{2} + \\\\frac{iH^{3}t^{3}}{6} + ...)   by the Taylor series expansion.   This says that during the evolution of a quantum state, the Hamiltonin is applied over and over again to the system with a various number of repetitions. The first term is the identity matrix so the system doesn't change at first, but in the second term the Hamiltonian is applied twice. For practical implementations, the series has to be truncated ( \\\\sum_{n \\\\mathop = 0}^N \\\\frac{(-iHt)^{n}}{n} where the bigger the N, the more accurate the simulation. \\n=== Quantum Walk ===\\nIn the quantum walk, a unitary operation whose spectrum is related to the Hamiltonian in implemented then the Quantum phase estimation algorithm is used to adjust the eigenvalues. This makes it unnecessary to decompose the Hamiltonian into a sum-of-terms like the Trotter-Suzuki methods.  \\n=== Quantum signal processing ===\\nThe quantum signal processing algorithm works by transducing the eigenvalues of the Hamiltonian into an ancilla qubit,  transforming the eigenvalues with single qubit rotations and finally projecting the ancilla.  It has been proved to be optimal in query complexity when it comes to Hamiltonian simulation. \\n== Complexity ==\\nThe table of the complexities of the Hamiltonian simulation algorithms mentioned above. The Hamiltonian simulation can be studied in two ways. This depends of how the hamiltonian is given. If it is given explicitly, then gate complexity matters more than query complexity. If the Hamiltonian is described as an Oracle (black box) then the number of queries to the oracle is more important than the gate count of the circuit. The following table shows the gate and query complexity of the previously mentioned techniques.\\nWhere ||H_{max}|| is the largest entry of H.\\n==References==\\nQuantum information science\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "give_harder_level(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The 2019 Conference Carolinas Men's Volleyball Tournament was the men's volleyball tournament for Conference Carolinas during the 2019 NCAA Division I & II men's volleyball season. It was held April 13 through April 18, 2019 at campus sites. The winner received the conference's automatic bid to the 2019 NCAA Volleyball Tournament.\\n==Seeds==\\nEight of the nine teams are eligible for the postseason, with the highest seed hosting each round. Teams are seeded by record within the conference, with a tiebreaker system to seed teams with identical conference records.\\n==Schedule and results==\\n==Bracket==\\nThe win clinched Barton's second trip to the NCAA Tournament.\\n==References==\\n2019 Conference Carolinas men's volleyball season\\n2019 NCAA Division I & II men's volleyball season\\nVolleyball competitions in the United States\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny = wikipedia.page(\"New York City\")\n",
    "new_york_city = ny.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I should be able to test the effectiveness "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
